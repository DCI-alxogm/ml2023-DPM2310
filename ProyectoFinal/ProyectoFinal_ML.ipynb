{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pEDPFITQt_Cx",
        "baBs7iH5uAnn",
        "E0a-iKzouCgZ"
      ],
      "authorship_tag": "ABX9TyMYMO2y6KeL5VHj9/mXdjC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCI-alxogm/ml2023-DPM2310/blob/main/ProyectoFinal/ProyectoFinal_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Se debe entregar una primera versión el 8 de Diciembre, pero se aceptan\n",
        " modificaciones en cualquier momento entre el 8 y el 14 de Diciembre,\n",
        "\n",
        "* Elegir un problema a resolver usando redes neuronales, ya sea de clasificación o regresión. Implementar la solución en tensorflow, se debe discutir la elección de los hiperparametros.\n",
        "\n",
        "* Es posible usar una arquitectura, o incluso una red ya entrenada, en un conjunto de datos nuevo, pero en el proyecto se debe explicar cada elemento de la red, la elección de los hiperparametros, etc etc..\n",
        "\n",
        "* Se puede usar cualquier tipo de red,  aunque no se haya visto en clase sin embargo se debe explicar las particularidades de la red en el proyecto.\n",
        "\n",
        "* Se debe entregar tanto un reporte como el código usado, link al github, reproducible en colab de preferencia. También se debe entregar reporte con introducción, desarrollo, conclusiones, etc. Es posible entregar el reporte junto con el código en un solo notebook siempre y cuando este sea realmente presentable, Ejemplo: https://github.com/desihub/tutorials/blob/main/getting_started/intro_to_DESI_EDR_files.ipynb\n"
      ],
      "metadata": {
        "id": "_4JOkEWcue8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proyecto Final Machine Learning**\n",
        "\n",
        "# **Clasificacion de Imagenes**: Piedra, Papel o Tijeras\n",
        "\n",
        "Universidad de Guanajuato, Division de Ciencias e Ingenierias\n",
        "\n",
        "Mtra. Alma Xochitl\n",
        "\n",
        "Alumno. Diego Paniagua Molina"
      ],
      "metadata": {
        "id": "cg5XujKtmP2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabla de Contenidos\n",
        "\n",
        "1. [Introducción](#intro)\n",
        "2. [Marco Teorico](#marco)\n",
        "3. [Metodologia](#metodologia)\n",
        "\n",
        "  3.1 [Definicion del Problema](#problema)\n",
        "\n",
        "  3.2 [Conjunto de Datos](#datos)\n",
        "\n",
        "  3.3 [Procesamiento de Datos](#procesamiento)\n",
        "\n",
        "  3.4 [Diseño de la Red Neuronal](#red)\n",
        "\n",
        "  3.5 [Eleccion de Hiperparametros](#hiperparametros)\n",
        "\n",
        "  3.6 [Entrenamiento del Modelo](#entrenamiento)\n",
        "\n",
        "  3.7 [Evaluacion del Modelo](#evaluacion)\n",
        "\n",
        "  3.8 [Implementacion en TensorFlow](#implementacion)\n",
        "\n",
        "4. [Resultados y Discusion](#resultados)  \n",
        "5. [Conclusiones](#conclusiones)\n",
        "6. [Referencias](#referencias)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JF1ZBfHOr6YJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción <a name=\"intro\"></a>\n",
        "El **Machine Learning** o aprendizaje automático posibilita la identificación de patrones en los datos basándose en algoritmos que clasifican cada factor según su grado de influencia aprendiendo y mejorando el proceso continuamente. [1]\n",
        "\n",
        "Las redes neuronales, en particular las redes neuronales convolucionales (CNN), han demostrado ser instrumentos valiosos para realizar tareas de clasificación o regresión a partir de datos de entrada, especialmente en el ámbito del procesamiento de imágenes. Este enfoque encuentra aplicaciones significativas, por ejemplo, en la identificación de objetos en una imagen, la clasificación de su contenido, o la realización de tareas más avanzadas como la segmentación y detección.\n",
        "\n",
        "Las CNN han revolucionado el campo del reconocimiento de imágenes al introducir un enfoque de procesamiento localizado y eficiente. A diferencia de las redes neuronales convencionales y otros algoritmos de clasificación de imágenes, las CNN utilizan filtros que operan en regiones específicas de la imagen, permitiendo la identificación de patrones locales de manera más efectiva.\n",
        "\n",
        "El proceso de entrenamiento de una CNN implica el uso de imágenes como entrada, representadas mediante píxeles, junto con sus respectivas etiquetas. Durante el aprendizaje, la red utiliza filtros para analizar y aplicar convoluciones a las imágenes. Cada filtro busca patrones específicos en pequeñas regiones, o ventanas, de la imagen, permitiendo que la CNN identifique características relevantes.\n",
        "\n",
        "En términos técnicos, las CNN toman tensores de forma que incluyen la altura y el ancho de la imagen en píxeles, así como el número de canales de color (1 para imágenes en blanco y negro o 3 para imágenes en color RGB). La salida de la red proporciona la clase a la que pertenece la imagen o la probabilidad asociada a esa clasificación.\n",
        "\n",
        "Este enfoque ha demostrado ser altamente eficaz en una variedad de aplicaciones de visión por computadora y ha contribuido significativamente al progreso en el campo del reconocimiento de patrones en imágenes.\n",
        "\n",
        "![Error](https://drive.google.com/uc?id=1b6ZLN30hunm1EOK5-jRK3rydAEdC0rhd)\n",
        "\n",
        "Imagen: https://nafizshahriar.medium.com/what-is-convolutional-neural-network-cnn-deep-learning-b3921bdd82d5 [2].\n",
        "\n",
        "El objetivo de este proyecto es analizar un conjunto de imagenes del famoso juego piedra, papel o tijeras mediante el uso de las CNN para realizar la tarea de clasificar dichas imagenes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QC7sY0V0sfhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marco Teorico <a name=\"marco\"></a>\n",
        "\n",
        "Las CNN tienen multiples aplicaciones, en este proyecto se estara estudiando especificamente la **clasificacion de imagenes**.  \n",
        "\n",
        "*Clasificación de imágenes*: la clasificación de imágenes es la tarea de asignar una etiqueta de clase a una imagen de entrada. Las CNN pueden entrenarse en grandes conjuntos de datos de imágenes etiquetadas para aprender las relaciones entre los píxeles de la imagen y las etiquetas de clase, y luego aplicarse a imágenes nuevas e invisibles para hacer una predicción.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uF0Duy5ft8J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metodologia <a name=\"metodologia\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pEDPFITQt_Cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definicion del Problema <a name=\"problema\"></a>\n"
      ],
      "metadata": {
        "id": "T4g3-yTBuPqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto de Datos <a name=\"datos\"></a>\n",
        "\n",
        "Los datos utilizados provienen de https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors [3]. Este conjunto de datos contiene imágenes de gestos con las manos del juego piedra, papel y tijera.\n"
      ],
      "metadata": {
        "id": "baBs7iH5uAnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos Kaggle para cargar el conjunto de datos en Google Colab.\n",
        "\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "kgsTU5oma6Uj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos nuestra clave API de Kaggle.\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "bHQzQaNkonUm",
        "outputId": "ae5f8873-90b6-4551-aeae-1b50beaf35a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1fe5445c-0f66-4ddf-a5b0-479e2c568cdb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1fe5445c-0f66-4ddf-a5b0-479e2c568cdb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"diegopaniaguamolina\",\"key\":\"2fc93b31e45aced11c3967cd95f51964\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Las líneas siguientes crean un directorio '.kaggle' en el directorio de inicio del usuario,\n",
        "# copian el archivo de la clave API cargada en el paso anterior en ese directorio,\n",
        "# y luego establecen permisos restrictivos para garantizar la seguridad de la clave.\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdKteuhvq5FQ",
        "outputId": "c822f014-3b57-4057-bfe7-f76275062644"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el dataset desde Kaggle.\n",
        "\n",
        "! kaggle datasets download -d 'drgfreeman/rockpaperscissors'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZVb2AeZrAxF",
        "outputId": "d87dd314-b2ef-484e-db7c-e9bed635c7ce"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading rockpaperscissors.zip to /content\n",
            " 97% 296M/306M [00:02<00:00, 196MB/s]\n",
            "100% 306M/306M [00:02<00:00, 147MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos el archivo presente en el directorio actual.\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbpLzU2hsSXS",
        "outputId": "0a368f3b-60d1-4d3c-b946-cba4a1caa598"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  kaggle.json  rockpaperscissors.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimimos el dataset .zip\n",
        "\n",
        "!unzip -q rockpaperscissors.zip"
      ],
      "metadata": {
        "id": "6YAx84gMtaLM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora tenemos las carpetas que se encontraban dentro del archivo .zip desplegadas.\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvEs9cCoui9g",
        "outputId": "5dc6a8f8-ca75-4a14-b608-4ee26c614dff"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t     paper\t\t       rock\t\t      rps-cv-images  scissors\n",
            "kaggle.json  README_rpc-cv-images.txt  rockpaperscissors.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento de Datos <a name=\"procesamiento\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E0a-iKzouCgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerias necesarias.\n",
        "\n",
        "import zipfile,os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "from keras import optimizers\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "KDMw20IbvtBy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos la version de TensorFlow\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqliRdRbwM8g",
        "outputId": "ada6e53c-39e6-4f8e-d14e-91f7c351e2db"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mediante la libreria 'os' usamos la funcion 'os.listdir' la cual\n",
        "# nos devuelve una lista de nombres de archivos en el directorio actual.\n",
        "\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o68bqah6wkcD",
        "outputId": "e65e0584-b595-4ca0-c081-ac871c2944a7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'scissors',\n",
              " 'rps-cv-images',\n",
              " 'rockpaperscissors.zip',\n",
              " 'kaggle.json',\n",
              " 'drive',\n",
              " 'paper',\n",
              " 'README_rpc-cv-images.txt',\n",
              " 'rock',\n",
              " '.ipynb_checkpoints',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos el archivo particular que queremos explorar.\n",
        "\n",
        "os.listdir('rps-cv-images')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw_Rx3tYxGLE",
        "outputId": "239c76f8-76f5-482b-af70-8b54e7f14a02"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scissors', 'paper', 'README_rpc-cv-images.txt', 'rock']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las carpetas que contienen las imágenes para cada categoría.\n",
        "\n",
        "rps_image = 'rps-cv-images'\n",
        "piedra_im = 'rock'\n",
        "papel_im = 'paper'\n",
        "tijeras_im = 'scissors'\n",
        "\n",
        "# Listamos los archivos en cada una de las carpetas correspondientes a las categorías de \"rock\", \"paper\" y \"scissors\".\n",
        "\n",
        "piedra_data = os.listdir(os.path.join(rps_image, piedra_im))\n",
        "papel_data = os.listdir(os.path.join(rps_image, papel_im))\n",
        "tijeras_data = os.listdir(os.path.join(rps_image, tijeras_im))\n",
        "\n",
        "# Imprimimos el número de imágenes en cada categoría.\n",
        "\n",
        "print('Numero de imagenes de Piedra:', len(piedra_data))\n",
        "print('Numero de imagenes de Papel:', len(papel_data))\n",
        "print('Numero de imagenes de Tijeras:', len(tijeras_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si9x-OW1xRLR",
        "outputId": "a77f1652-57ab-4b8b-d6fc-c1dfe07f7a7b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de imagenes de Piedra: 726\n",
            "Numero de imagenes de Papel: 712\n",
            "Numero de imagenes de Tijeras: 750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, generamos lotes de datos aumentados durante el entrenamiento de nuestro modelo utilizando la clase ImageDataGenerator de TensorFlow/Keras. Es importante realizar esto ya que tiene varios propósitos y beneficios:\n",
        "\n",
        "* **Aumentar la Variabilidad del Conjunto de Datos**: La generación de datos aumentados introduce variaciones en las imágenes originales mediante transformaciones como rotaciones, desplazamientos, zoom, inversiones, etc. Esto ayuda a enriquecer el conjunto de datos y proporciona al modelo más variedad para aprender patrones.\n",
        "\n",
        "* **Prevención del Sobreajuste**: El sobreajuste ocurre cuando un modelo aprende demasiado bien los detalles específicos de los datos de entrenamiento, pero no generaliza bien a nuevos datos. La aumentación de datos ayuda a prevenir el sobreajuste al presentar variaciones en el conjunto de entrenamiento, lo que permite al modelo aprender patrones más robustos.\n",
        "\n",
        "* **Mejora de la Generalización**: Al exponer el modelo a diferentes variaciones de los datos, se espera que el modelo generalice mejor a nuevos datos del mundo real que pueden tener ciertas variaciones o perturbaciones.\n",
        "\n",
        "* **Reducción del Riesgo de Sobreentrenamiento**: El aumento de datos también puede ayudar a reducir el riesgo de sobreentrenamiento, especialmente cuando el tamaño del conjunto de datos es limitado. La aumentación proporciona al modelo una mayor cantidad de ejemplos \"virtuales\" para aprender.\n",
        "\n",
        "* **Mejora de la Robustez del Modelo**: Un modelo entrenado con datos aumentados puede ser más robusto a pequeñas variaciones en las imágenes de entrada, lo que puede ser beneficioso en situaciones del mundo real donde las condiciones pueden variar.\n",
        "\n",
        "* **Entrenamiento Eficiente**: La generación de datos aumentados en tiempo real permite entrenar modelos con conjuntos de datos más pequeños, ya que cada imagen puede ser transformada de diversas maneras, generando virtualmente nuevos ejemplos de entrenamiento.\n",
        "\n",
        "* **Mejora del Rendimiento del Modelo**: En muchos casos, la incorporación de aumentación de datos durante el entrenamiento ha demostrado mejorar el rendimiento y la precisión de los modelos de aprendizaje profundo.\n",
        "\n",
        "En resumen, generar lotes de datos aumentados durante el entrenamiento es una estrategia efectiva para mejorar la capacidad de generalización de un modelo y mitigar problemas como el sobreajuste, especialmente en problemas de visión por computadora donde las variaciones en los datos de entrada son comunes. A continuacion se muestra el proeso para realizar este metodo."
      ],
      "metadata": {
        "id": "kTbnQMn006pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos el generador de imagenes y dividimos el conjunto de datos en\n",
        "# conjuntos de entrenamiento y validación, reservando el 40% para validación y\n",
        "# normalizamos los valores de píxeles de las imágenes para que estén en el rango\n",
        "# [0, 1]. Dividimos por 255 para llevar los valores a la escala 0-1.\n",
        "\n",
        "img_generator = ImageDataGenerator(validation_split = 0.4,\n",
        "                                   rescale = 1./255)\n",
        "\n",
        "# Ahora el generador de entrenamiento, cargamos las imágenes, las redimensionamos\n",
        "\n",
        "train_generator = img_generator.flow_from_directory(rps_image,\n",
        "                                                    target_size = (150,150),\n",
        "                                                    batch_size = 4,\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    subset = 'training')\n",
        "\n",
        "validation_generator = img_generator.flow_from_directory(rps_image,\n",
        "                                                         target_size = (150,150),\n",
        "                                                         batch_size = 4,\n",
        "                                                         class_mode = 'categorical',\n",
        "                                                         subset = 'validation')"
      ],
      "metadata": {
        "id": "dBmxBGZXzxaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diseño de la Red Neuronal <a name=\"red\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "aVQaFVyYuDuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eleccion de Hiperparametros <a name=\"hiperparametros\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vKdZuOvjuExk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del Modelo <a name=\"entrenamiento\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0eFLuocuFxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluacion del Modelo <a name=\"evaluacion\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "D1ISsKmUuG0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementacion en TensorFlow <a name=\"implementacion\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "hjkqtnLzuIKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados y Discusion <a name=\"resultados\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zrfq9CFvuJ4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones <a name=\"conclsuiones\"></a>\n"
      ],
      "metadata": {
        "id": "kI_oOvuHuLN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias <a name=\"referencias\"></a>\n",
        "\n",
        "[1] https://www.lisdatasolutions.com/es/blog/deep-learning-clasificando-imagenes-con-redes-neuronales/\n",
        "\n",
        "[2] https://nafizshahriar.medium.com/what-is-convolutional-neural-network-cnn-deep-learning-b3921bdd82d5\n",
        "\n",
        "[3] Dataset. https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors"
      ],
      "metadata": {
        "id": "bc1s3i25uqCd"
      }
    }
  ]
}