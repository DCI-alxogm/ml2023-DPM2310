{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QC7sY0V0sfhm",
        "uF0Duy5ft8J-",
        "0Y-S7RqbRr_o",
        "hKFGIrHlRkUt",
        "xVQ73p7xPKxq",
        "SFOk2xd1NRm_",
        "ncMqTRJrJIp-",
        "pEDPFITQt_Cx",
        "T4g3-yTBuPqP",
        "baBs7iH5uAnn",
        "E0a-iKzouCgZ",
        "aVQaFVyYuDuQ",
        "vKdZuOvjuExk",
        "bc1s3i25uqCd"
      ],
      "authorship_tag": "ABX9TyPxq4USwGPTAhI85EKpS+bJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCI-alxogm/ml2023-DPM2310/blob/main/ProyectoFinal/ProyectoFinal_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Se debe entregar una primera versión el 8 de Diciembre, pero se aceptan\n",
        " modificaciones en cualquier momento entre el 8 y el 14 de Diciembre,\n",
        "\n",
        "* Elegir un problema a resolver usando redes neuronales, ya sea de clasificación o regresión. Implementar la solución en tensorflow, se debe discutir la elección de los hiperparametros.\n",
        "\n",
        "* Es posible usar una arquitectura, o incluso una red ya entrenada, en un conjunto de datos nuevo, pero en el proyecto se debe explicar cada elemento de la red, la elección de los hiperparametros, etc etc..\n",
        "\n",
        "* Se puede usar cualquier tipo de red,  aunque no se haya visto en clase sin embargo se debe explicar las particularidades de la red en el proyecto.\n",
        "\n",
        "* Se debe entregar tanto un reporte como el código usado, link al github, reproducible en colab de preferencia. También se debe entregar reporte con introducción, desarrollo, conclusiones, etc. Es posible entregar el reporte junto con el código en un solo notebook siempre y cuando este sea realmente presentable, Ejemplo: https://github.com/desihub/tutorials/blob/main/getting_started/intro_to_DESI_EDR_files.ipynb\n"
      ],
      "metadata": {
        "id": "_4JOkEWcue8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proyecto Final Machine Learning**\n",
        "\n",
        "# **Clasificacion de Imagenes**: Piedra, Papel o Tijeras\n",
        "\n",
        "Universidad de Guanajuato, Division de Ciencias e Ingenierias\n",
        "\n",
        "Mtra. Alma Xochitl\n",
        "\n",
        "Alumno. Diego Paniagua Molina"
      ],
      "metadata": {
        "id": "cg5XujKtmP2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "eNTurfJhYUNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabla de Contenidos\n",
        "\n",
        "1. [Introducción](#intro)\n",
        "2. [Marco Teorico](#marco)\n",
        "\n",
        ">2.1 [Clasificacion de Imagenes](#clas)\n",
        "\n",
        ">2.2 [Arqitectura de una CNN](#arqui)\n",
        "\n",
        ">2.3 [Operacion Convolucin](#conv)\n",
        "\n",
        ">2.4 [Funciones de Activacion](#funcion)\n",
        "\n",
        ">2.5 [Capas de una CNN](#capas)\n",
        "\n",
        "3. [Metodologia](#metodologia)\n",
        "\n",
        ">3.1 [Definicion del Problema](#problema)\n",
        "\n",
        ">3.2 [Conjunto de Datos](#datos)\n",
        "\n",
        ">3.3 [Procesamiento de Datos](#procesamiento)\n",
        "\n",
        ">3.4 [Diseño de la Red Neuronal](#red)\n",
        "\n",
        ">3.5 [Eleccion de Hiperparametros](#hiperparametros)\n",
        "\n",
        ">3.6 [Entrenamiento del Modelo](#entrenamiento)\n",
        "\n",
        ">3.7 [Evaluacion del Modelo](#evaluacion)\n",
        "\n",
        "4. [Resultados y Discusion](#resultados)  \n",
        "5. [Conclusiones](#conclusiones)\n",
        "6. [Referencias](#referencias)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JF1ZBfHOr6YJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n81YiQRhYiyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción <a name=\"intro\"></a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QC7sY0V0sfhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El **Machine Learning** o aprendizaje automático posibilita la identificación de patrones en los datos basándose en algoritmos que clasifican cada factor según su grado de influencia aprendiendo y mejorando el proceso continuamente. [1]\n",
        "\n",
        "Las **redes neuronales**, en particular las **redes neuronales convolucionales** (*CNN*), han demostrado ser instrumentos valiosos para realizar tareas de **clasificación** o **regresión** a partir de datos de entrada, especialmente en el ámbito del **procesamiento de imágenes**. Este enfoque encuentra aplicaciones significativas, por ejemplo, en la identificación de objetos en una imagen, la clasificación de su contenido, o la realización de tareas más avanzadas como la segmentación y detección.\n",
        "\n",
        "Las CNN han revolucionado el campo del reconocimiento de imágenes al introducir un enfoque de procesamiento localizado y eficiente. A diferencia de las redes neuronales convencionales y otros algoritmos de clasificación de imágenes, las CNN utilizan **filtros** que operan en regiones específicas de la imagen, permitiendo la identificación de patrones locales de manera más efectiva.\n",
        "\n",
        "El proceso de entrenamiento de una CNN implica el uso de imágenes como entrada, representadas mediante píxeles, junto con sus respectivas etiquetas. Durante el aprendizaje, la red utiliza filtros para analizar y aplicar convoluciones a las imágenes. Cada filtro busca patrones específicos en pequeñas regiones, o ventanas, de la imagen, permitiendo que la CNN identifique características relevantes.\n",
        "\n",
        "En términos técnicos, las CNN toman **tensores** de forma que incluyen la altura y el ancho de la imagen en píxeles, así como el número de canales de color (1 para imágenes en blanco y negro o 3 para imágenes en color RGB). La salida de la red proporciona la clase a la que pertenece la imagen o la probabilidad asociada a esa clasificación.\n",
        "\n",
        "Este enfoque ha demostrado ser altamente eficaz en una variedad de aplicaciones de visión por computadora y ha contribuido significativamente al progreso en el campo del reconocimiento de patrones en imágenes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zdOksfukRbm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuacion se muestra un ejemplo grafico de la arquitectura de una CNN:\n",
        "\n",
        "![Error](https://drive.google.com/uc?id=1b6ZLN30hunm1EOK5-jRK3rydAEdC0rhd)\n",
        "\n",
        "Imagen: https://nafizshahriar.medium.com/what-is-convolutional-neural-network-cnn-deep-learning-b3921bdd82d5 [2].\n",
        "\n"
      ],
      "metadata": {
        "id": "emBuKvPQRkrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este proyecto es analizar un conjunto de imagenes del famoso juego piedra, papel o tijeras mediante el uso de las CNN para realizar la tarea de clasificar dichas imagenes.\n"
      ],
      "metadata": {
        "id": "CIpPSQoZRmBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marco Teorico <a name=\"marco\"></a>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uF0Duy5ft8J-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificacion de Imagenes <a name=\"clas\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "0Y-S7RqbRr_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las CNN tienen multiples aplicaciones, en este proyecto se estara estudiando especificamente la **clasificacion de imagenes**.  \n",
        "\n",
        "* *Clasificación de imágenes*: es la tarea de asignar una etiqueta de clase a una imagen de entrada. Las CNN pueden entrenarse en grandes conjuntos de datos de imágenes etiquetadas para aprender las relaciones entre los píxeles de la imagen y las etiquetas de clase, y luego aplicarse a imágenes nuevas e invisibles para hacer una predicción."
      ],
      "metadata": {
        "id": "XwP9wdThNsgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arquitectura de una CNN <a name=\"arqui\"></a>"
      ],
      "metadata": {
        "id": "hKFGIrHlRkUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La **arquitectura** de una red neuronal para la clasificación de imágenes implica la organización de capas diseñadas para aprender y reconocer patrones visuales en datos de imágenes.\n",
        "\n",
        "Generalmente, incluye capas convolucionales para la extracción de características, capas de pooling para reducir la dimensionalidad, una capa de aplanamiento y capas densas para la clasificación. La última capa suele tener activación softmax para la salida de probabilidades de las clases. Los detalles específicos, como el número de capas, filtros y neuronas, así como las funciones de activación, se ajustan según la complejidad del problema y la naturaleza de los datos de imágenes."
      ],
      "metadata": {
        "id": "tTqcFZ7QRpsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operacion Convolucion <a name=\"conv\"></a>"
      ],
      "metadata": {
        "id": "xVQ73p7xPKxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La **convolución** es una operación matemática sencilla que se suele utilizar para el tratamiento y el reconocimiento de imágenes. En una imagen, su efecto se asimila a un filtrado, a continuacion podemos ver su funcionamiento [5].\n",
        "\n",
        "\n",
        "![Error](https://drive.google.com/uc?id=1Lmrl_ZaW0DnnS51NQT-9ssmqGQkEu4X6)\n",
        "\n",
        "\n",
        "1. En primer lugar, se define el tamaño de la ventana de filtro situada en la parte superior izquierda.\n",
        "\n",
        "2. La ventana de filtro, que representa la característica, se desplaza progresivamente de izquierda a derecha un determinado número de casillas definido previamente (el paso) hasta llegar al final de la imagen.\n",
        "\n",
        "3. En cada porción de imagen que encuentra, se efectúa el cálculo de convolución permitiendo obtener en la salida una tarjeta de activación o feature map que indica dónde están localizadas las features en la imagen: cuanto más arriba esté la feature, más se parecerá a ella la porción de imagen barrida."
      ],
      "metadata": {
        "id": "4_GJVc_8PVB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de Activacion <a name=\"funcion\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "SFOk2xd1NRm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La **función de activación** se encarga de devolver una salida a partir de un valor de entrada, normalmente el conjunto de valores de salida en un rango determinado como (0,1) o (-1,1).\n",
        "\n",
        "Se buscan funciones que las derivadas sean simples, para minimizar con ello el coste computacional [6]."
      ],
      "metadata": {
        "id": "kH79OhpmNxX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las dos funciones de activacion utilizadas en este prroyecto se definen a continuacion:\n",
        "\n",
        "1.   **ReLU - Rectified Lineal Unit**. La función ReLU transforma los valores introducidos anulando los valores negativos y dejando los positivos tal y como entran, esta funcion tiene la siguiente forma:\n",
        "\n",
        "   $f(x) = \\max(0, x)$\n",
        "\n",
        "\n",
        "2.   **Softmax - Rectified Lineal Unit**. La función Softmax transforma las salidas a una representación en forma de probabilidades, de tal manera que el sumatorio de todas las probabilidades de las salidas de 1, esta funcion tiene la siguiente forma:\n",
        "\n",
        "   $f(x)_i = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $\n"
      ],
      "metadata": {
        "id": "H_QmjmZdN2Sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capas de una CNN <a name=\"capas\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "ncMqTRJrJIp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar esta clasificacion se hara uso de multiples **capas** de distintos tipos en la CNN. Las capas en una CNN desempeñan roles específicos en el procesamiento de información.\n",
        "\n",
        "1. *Capa de entrada*:Representa la entrada de datos a la red y su función es definir las dimensiones de la entrada, como el tamaño de la imagen y el número de canales de color.\n",
        "\n",
        "2. *Capa convolucional*: son las encargadas de aplicar la convolución a nuestras imágenes de entrada para encontrar los patrones que más tarde permitirán clasificarla [4].\n",
        "\n",
        "3. *Capa de pooling*: se utilizan para ir reduciendo el tamaño de nuestros mapas de activaciones, ya que de otra forma no sería posible ejecutarlos en GPUs. Además, también ayuda a reducir el overfitting [4].\n",
        "\n",
        "4. *Capa de aplanamiento*: Transforma la salida tridimensional de las capas anteriores en un vector unidimensional y prepara los datos para la entrada a capas densas.\n",
        "\n",
        "5. *Capa Densa*: Realiza operaciones densas (totalmente conectadas) en los datos aplanados, se conocen como \"las de siempre\", debido a que son las más básicas y utilizadas [4].\n",
        "\n",
        "6. *Capa de Salida*: Produce la salida final de la red."
      ],
      "metadata": {
        "id": "hK9OhgbHNvjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metodologia <a name=\"metodologia\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pEDPFITQt_Cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definicion del Problema <a name=\"problema\"></a>\n"
      ],
      "metadata": {
        "id": "T4g3-yTBuPqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este proyecto consiste en la tarea de **clasificación de imágenes** en tres clases distintas (piedra, papel o tijeras). Entonces, buscamos desarrollar un modelo de aprendizaje capaz de asignar de manera precisa y eficiente cada imagen de entrada a una de las tres categorías predefinidas. La clasificación se fundamenta en la capacidad del modelo para aprender patrones y características distintivas presentes en las imágenes que caracterizan cada clase. El objetivo es lograr una generalización efectiva a partir de un conjunto de datos de entrenamiento, de manera que el modelo pueda realizar predicciones precisas en imágenes no vistas previamente. La correcta clasificación de estas imágenes se considera esencial para el éxito del modelo y, por ende, para la resolución del problema planteado."
      ],
      "metadata": {
        "id": "v8cR7P8_YAmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto de Datos <a name=\"datos\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "baBs7iH5uAnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los datos utilizados provienen de https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors [3]. Este conjunto de datos contiene imágenes de gestos con las manos del juego piedra, papel y tijera."
      ],
      "metadata": {
        "id": "DkIz8xkYQawC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos Kaggle para cargar el conjunto de datos en Google Colab.\n",
        "\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "kgsTU5oma6Uj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos nuestra clave API de Kaggle.\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "bHQzQaNkonUm",
        "outputId": "e9779ad9-47fe-4894-fd6a-e2c5f3075894"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3c26b05-007a-4739-836e-6f8930ba0f53\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3c26b05-007a-4739-836e-6f8930ba0f53\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"diegopaniaguamolina\",\"key\":\"2fc93b31e45aced11c3967cd95f51964\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Las líneas siguientes crean un directorio '.kaggle' en el directorio de inicio del usuario,\n",
        "# copian el archivo de la clave API cargada en el paso anterior en ese directorio,\n",
        "# y luego establecen permisos restrictivos para garantizar la seguridad de la clave.\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "QdKteuhvq5FQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el dataset desde Kaggle.\n",
        "\n",
        "! kaggle datasets download -d 'drgfreeman/rockpaperscissors'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZVb2AeZrAxF",
        "outputId": "8140709a-6edf-485a-fca4-530e9450200b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading rockpaperscissors.zip to /content\n",
            " 94% 287M/306M [00:02<00:00, 144MB/s]\n",
            "100% 306M/306M [00:02<00:00, 142MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos el archivo presente en el directorio actual.\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbpLzU2hsSXS",
        "outputId": "f6f5d86c-ac11-46d2-b9a6-82e0f4433550"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json  rockpaperscissors.zip  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimimos el dataset .zip\n",
        "\n",
        "!unzip -q rockpaperscissors.zip"
      ],
      "metadata": {
        "id": "6YAx84gMtaLM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora tenemos las carpetas que se encontraban dentro del archivo .zip desplegadas.\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvEs9cCoui9g",
        "outputId": "33a596f6-2f60-43ce-8534-1bad1b20f4e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json  README_rpc-cv-images.txt  rockpaperscissors.zip  sample_data\n",
            "paper\t     rock\t\t       rps-cv-images\t      scissors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento de Datos <a name=\"procesamiento\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E0a-iKzouCgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerias necesarias.\n",
        "\n",
        "import zipfile,os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "from keras import optimizers\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "KDMw20IbvtBy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos la version de TensorFlow\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqliRdRbwM8g",
        "outputId": "14bf457d-5d7c-48f9-f858-31550b6286c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mediante la libreria 'os' usamos la funcion 'os.listdir' la cual\n",
        "# nos devuelve una lista de nombres de archivos en el directorio actual.\n",
        "\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o68bqah6wkcD",
        "outputId": "1b2a39be-d67b-4971-c5fd-bfbc2b2eac9d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'scissors',\n",
              " 'rps-cv-images',\n",
              " 'rockpaperscissors.zip',\n",
              " 'kaggle.json',\n",
              " 'paper',\n",
              " 'README_rpc-cv-images.txt',\n",
              " 'rock',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos el archivo particular que queremos explorar.\n",
        "\n",
        "os.listdir('rps-cv-images')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw_Rx3tYxGLE",
        "outputId": "9162e9d3-bd92-451f-e388-1905340039d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scissors', 'paper', 'README_rpc-cv-images.txt', 'rock']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las carpetas que contienen las imágenes para cada categoría.\n",
        "\n",
        "rps_image = 'rps-cv-images'\n",
        "piedra_im = 'rock'\n",
        "papel_im = 'paper'\n",
        "tijeras_im = 'scissors'\n",
        "\n",
        "# Listamos los archivos en cada una de las carpetas correspondientes a las categorías de \"rock\", \"paper\" y \"scissors\".\n",
        "\n",
        "piedra_data = os.listdir(os.path.join(rps_image, piedra_im))\n",
        "papel_data = os.listdir(os.path.join(rps_image, papel_im))\n",
        "tijeras_data = os.listdir(os.path.join(rps_image, tijeras_im))\n",
        "\n",
        "# Imprimimos el número de imágenes en cada categoría.\n",
        "\n",
        "print('Numero de imagenes de Piedra:', len(piedra_data))\n",
        "print('Numero de imagenes de Papel:', len(papel_data))\n",
        "print('Numero de imagenes de Tijeras:', len(tijeras_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si9x-OW1xRLR",
        "outputId": "b8da3441-53dd-4168-e660-abd8a5896403"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de imagenes de Piedra: 726\n",
            "Numero de imagenes de Papel: 712\n",
            "Numero de imagenes de Tijeras: 750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, para el procesamiento de datos generamos lotes de datos aumentados durante el entrenamiento de nuestro modelo utilizando la clase ImageDataGenerator de TensorFlow/Keras. Es importante realizar esto ya que tiene varios propósitos y beneficios:\n",
        "\n",
        "* **Aumentar la Variabilidad del Conjunto de Datos**: La generación de datos aumentados introduce variaciones en las imágenes originales mediante transformaciones como rotaciones, desplazamientos, zoom, inversiones, etc. Esto ayuda a enriquecer el conjunto de datos y proporciona al modelo más variedad para aprender patrones.\n",
        "\n",
        "* **Prevención del Sobreajuste**: El sobreajuste ocurre cuando un modelo aprende demasiado bien los detalles específicos de los datos de entrenamiento, pero no generaliza bien a nuevos datos. La aumentación de datos ayuda a prevenir el sobreajuste al presentar variaciones en el conjunto de entrenamiento, lo que permite al modelo aprender patrones más robustos.\n",
        "\n",
        "* **Mejora de la Generalización**: Al exponer el modelo a diferentes variaciones de los datos, se espera que el modelo generalice mejor a nuevos datos del mundo real que pueden tener ciertas variaciones o perturbaciones.\n",
        "\n",
        "* **Reducción del Riesgo de Sobreentrenamiento**: El aumento de datos también puede ayudar a reducir el riesgo de sobreentrenamiento, especialmente cuando el tamaño del conjunto de datos es limitado. La aumentación proporciona al modelo una mayor cantidad de ejemplos \"virtuales\" para aprender.\n",
        "\n",
        "* **Mejora de la Robustez del Modelo**: Un modelo entrenado con datos aumentados puede ser más robusto a pequeñas variaciones en las imágenes de entrada, lo que puede ser beneficioso en situaciones del mundo real donde las condiciones pueden variar.\n",
        "\n",
        "* **Entrenamiento Eficiente**: La generación de datos aumentados en tiempo real permite entrenar modelos con conjuntos de datos más pequeños, ya que cada imagen puede ser transformada de diversas maneras, generando virtualmente nuevos ejemplos de entrenamiento.\n",
        "\n",
        "* **Mejora del Rendimiento del Modelo**: En muchos casos, la incorporación de aumentación de datos durante el entrenamiento ha demostrado mejorar el rendimiento y la precisión de los modelos de aprendizaje profundo.\n",
        "\n",
        "En resumen, generar lotes de datos aumentados durante el entrenamiento es una estrategia efectiva para mejorar la capacidad de generalización de un modelo y mitigar problemas como el sobreajuste, especialmente en problemas de visión por computadora donde las variaciones en los datos de entrada son comunes. A continuacion se muestra el proeso para realizar este metodo:"
      ],
      "metadata": {
        "id": "kTbnQMn006pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos el generador de imagenes y dividimos el conjunto de datos en\n",
        "# conjuntos de entrenamiento y validación, reservando el 40% para validación y\n",
        "# normalizamos los valores de píxeles de las imágenes para que estén en el rango\n",
        "# [0, 1]. Dividimos por 255 para llevar los valores a la escala 0-1.\n",
        "\n",
        "img_generator = ImageDataGenerator(validation_split = 0.4,\n",
        "                                   rescale = 1./255)\n",
        "\n",
        "# Ahora el generador de entrenamiento, cargamos las imágenes y las redimensionamos,\n",
        "# tambien especificamos que se generen lotes de 4 imágenes a la vez durante el\n",
        "# entrenamiento.\n",
        "\n",
        "train_generator = img_generator.flow_from_directory(rps_image,\n",
        "                                                    target_size = (150,150),\n",
        "                                                    batch_size = 4,\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    subset = 'training')\n",
        "\n",
        "# Por ultimo, de la misma forma para el generador de validacion.\n",
        "\n",
        "validation_generator = img_generator.flow_from_directory(rps_image,\n",
        "                                                         target_size = (150,150),\n",
        "                                                         batch_size = 4,\n",
        "                                                         class_mode = 'categorical',\n",
        "                                                         subset = 'validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBmxBGZXzxaF",
        "outputId": "3a6edcc2-3775-4d13-a920-c92593df272b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1314 images belonging to 3 classes.\n",
            "Found 874 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El mensaje mostrado hace referencia a que hay 1314 imágenes para datos de entrenamiento. Mientras que hay 874 datos de validación con las mismas tres clases que los datos de entrenamiento."
      ],
      "metadata": {
        "id": "QuMx9n1lPOVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diseño de la Red Neuronal <a name=\"red\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "aVQaFVyYuDuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora definimos la arquitectura de la CNN con ayuda de TensorFlow y Keras."
      ],
      "metadata": {
        "id": "_QxE-tEiQwlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura de la red neuronal para abordar el problema de clasificación de tres tipos de imágenes (piedra, papel o tijeras) se basa en consideraciones específicas relacionadas con las características del conjunto de datos que estamos utilizando y con requisitos expuestos del proyecto. La arquitectura propuesta se describe de la siguiente manera:"
      ],
      "metadata": {
        "id": "ZkkvnxoPW_kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arquitectura de la CNN.\n",
        "\n",
        "arquitectura = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape= (150,150,3)),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(64,(3,3), activation= 'relu' ),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(128,(3,3), activation= 'relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(256,(3,3), activation= 'relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(512,(3,3), activation= 'relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation= 'relu'),\n",
        "  tf.keras.layers.Dense(3, activation= 'softmax')\n",
        "])\n",
        "\n",
        "# Mostramos el resumen de las caracteristicas de la arquitectura.\n",
        "\n",
        "arquitectura.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpzAlMZoQlJR",
        "outputId": "f7c071ac-7246-4f4a-f0b8-b232440fdc8e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPooli  (None, 74, 74, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPooli  (None, 36, 36, 64)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPooli  (None, 17, 17, 128)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 15, 15, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPooli  (None, 7, 7, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 5, 5, 512)         1180160   \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPooli  (None, 2, 2, 512)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               1049088   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2619203 (9.99 MB)\n",
            "Trainable params: 2619203 (9.99 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La justificacion para elegir esta arquitectura particular es debido a que:\n",
        "\n",
        "* La **combinación** de capas convolucionales y de Max Pooling permite la extracción de patrones locales y la reducción de la dimensionalidad de las representaciones.\n",
        "\n",
        "* La estructura de la red sigue una **jerarquía de caracteristicas** que incrementa progresivamente el número de filtros en las capas convolucionales y reduce el tamaño de la representación con capas de Max Pooling. Esto facilita la captura de características locales y la combinación de información en niveles más altos.\n",
        "\n",
        "* Para **evitar sobreajuste** se incorporan capas de Max Pooling y Dropout para mitigar el riesgo de sobreajuste. Estas capas reducen la complejidad del modelo y proporcionan regularización durante el entrenamiento.\n",
        "\n",
        "* **Número de Clases**. La última capa densa tiene 3 neuronas con activación softmax, indicando que el modelo está diseñado para clasificar imágenes en tres clases, alineándose con la naturaleza de la tarea.\n",
        "\n",
        "* **Capacidad de Aprendizaje**. La arquitectura posee suficientes parámetros para aprender patrones complejos en datos de imágenes. Las capas densas al final permiten la combinación no lineal de características aprendidas.\n",
        "\n",
        "* **Capacidad Computacional**. La complejidad computacional de la arquitectura es manejable y se ajusta a los recursos disponibles para el entrenamiento del modelo.\n",
        "\n",
        "* **Experiencia Empírica**. La elección se basa en experimentos anteriores realizados por multiples personas y ajustes realizados en arquitecturas similares, demostrando eficacia en tareas de clasificación de imágenes."
      ],
      "metadata": {
        "id": "EaPQSN8JXJ1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esquematicamente la arquitectura tendria la siguiente forma:\n",
        "\n",
        "![Error](https://drive.google.com/uc?id=1D02xnG0DtlQkBaheDwbSBmn9lwqgOQNb)\n",
        "\n"
      ],
      "metadata": {
        "id": "Czl1P5kUbrXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizando de forma detallada capa por capa de nuestra CNN tenemos que:\n",
        "\n",
        "1. **Capa de Convolución 2D**:\n",
        "    \n",
        "    Esta capa tiene 32 filtros de tamaño 3x3, utiliza la función de activación ReLU y espera entradas de forma (150,150,3).\n",
        "\n",
        "2. **Capa de Max Pooling 2D**:\n",
        "\n",
        "    Esta capa reduce la resolución espacial de la representación de la capa anterior tomando el valor máximo en ventanas de 2x2.\n",
        "\n",
        "3. **Capa de Convolución 2D**:\n",
        "\n",
        "    Otra capa de convolución con 64 filtros de tamaño 3x3 y activación ReLU.\n",
        "\n",
        "4. **Capa de Max Pooling 2D**:\n",
        "\n",
        "    \" \"\n",
        "\n",
        "5. **Capa de Convolución 2D**:\n",
        "\n",
        "    \" \"\n",
        "\n",
        "6. **Capa de Max Pooling 2D**:\n",
        "\n",
        "    \" \"\n",
        "\n",
        "7. **Capa de Convolución 2D**:\n",
        "\n",
        "    \" \"\n",
        "8. **Capa de Max Pooling 2D**:\n",
        "\n",
        "    \" \"\n",
        "9. **Capa de Convolución 2D**:\n",
        "\n",
        "    \" \"\n",
        "10. **Capa de Max Pooling 2D**:\n",
        "\n",
        "    \" \"\n",
        "\n",
        "11. **Capa de Aplanamiento** (*Flatten*):\n",
        "\n",
        "    Esta capa transforma la salida 3D a un vector 1D antes de pasar a la capa densa.\n",
        "\n",
        "12. **Capa Densa** (*Fully Connected*):\n",
        "\n",
        "    Capa densa con 512 neuronas y activación ReLU.\n",
        "\n",
        "13. **Capa de Salida Densa**:\n",
        "\n",
        "    Por ultimo la capa densa de salida con 3 neuronas y activación softmax, adecuada para un problema de clasificación con 3 clases.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "gljgK6laUCqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eleccion de Hiperparametros <a name=\"hiperparametros\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vKdZuOvjuExk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del Modelo <a name=\"entrenamiento\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0eFLuocuFxW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xjrhcAAOq0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluacion del Modelo <a name=\"evaluacion\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "D1ISsKmUuG0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados y Discusion <a name=\"resultados\"></a>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zrfq9CFvuJ4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones <a name=\"conclsuiones\"></a>\n"
      ],
      "metadata": {
        "id": "kI_oOvuHuLN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias <a name=\"referencias\"></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "bc1s3i25uqCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] https://www.lisdatasolutions.com/es/blog/deep-learning-clasificando-imagenes-con-redes-neuronales/\n",
        "\n",
        "[2] https://nafizshahriar.medium.com/what-is-convolutional-neural-network-cnn-deep-learning-b3921bdd82d5\n",
        "\n",
        "[3] Dataset. https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors\n",
        "\n",
        "[4] https://keepcoding.io/blog/tipos-capas-red-neuronal-convolucional/\n",
        "\n",
        "[5] https://datascientest.com/es/convolutional-neural-network-es\n",
        "\n",
        "[6] https://www.diegocalvo.es/funcion-de-activacion-redes-neuronales/"
      ],
      "metadata": {
        "id": "7iMqJU83GUU3"
      }
    }
  ]
}